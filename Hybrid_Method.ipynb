{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are using ensemble method where we will be combining both CNN and LSTM and will be appling with these specification:\n",
        "\n",
        "1.   Activation Funtion: Sigmoid\n",
        "2.   Optimizer: NADAM\n",
        "\n",
        "\n",
        "*   cause these were the ones who gave maximum accuracy for out dataset.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-2oZxoAKiPcW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjrwj4Dxgixd",
        "outputId": "d50a2754-43fc-4083-ffc4-31aba0486406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-7402885108f7>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['anomaly'] = df['anomaly'].astype(int)\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "# from IPython.display import display\n",
        "url = \"https://springernature.figshare.com/ndownloader/files/34135113\"\n",
        "response = urlopen(url)\n",
        "with response as file:\n",
        "    dataset = pickle.load(file)\n",
        "\n",
        "df=pd.DataFrame(dataset)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "df['signal_pleth'] = df['signal_pleth'].apply(lambda x: np.mean(x))\n",
        "df['signal_ecg_i'] = df['signal_ecg_i'].apply(lambda x: np.mean(x))\n",
        "df['signal_ecg_ii'] = df['signal_ecg_ii'].apply(lambda x: np.mean(x))\n",
        "df['signal_ecg_iii'] = df['signal_ecg_iii'].apply(lambda x: np.mean(x))\n",
        "df['PSG_Abdomen'] = df['PSG_Abdomen'].apply(lambda x: np.mean(x))\n",
        "df['PSG_Flow'] = df['PSG_Flow'].apply(lambda x: np.mean(x))\n",
        "df['PSG_Position'] = df['PSG_Position'].apply(lambda x: np.mean(x))\n",
        "df['PSG_Snore'] = df['PSG_Snore'].apply(lambda x: np.mean(x))\n",
        "df['PSG_Thorax'] = df['PSG_Thorax'].apply(lambda x: np.mean(x))\n",
        "\n",
        "\n",
        "df.drop(['patient', 'timestamp_datetime'], axis=1)\n",
        "\n",
        "df = df.dropna()\n",
        "df['anomaly'] = df['anomaly'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['event_encoded'] = label_encoder.fit_transform(df['event'])\n",
        "\n",
        "# Calculate the number of unique classes\n",
        "num_classes = df['event_encoded'].nunique()\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    df[['HR(bpm)', 'SpO2(%)', 'PI(%)', 'RR(rpm)', 'PVCs(/min)', 'anomaly', 'signal_pleth', 'signal_ecg_i', 'signal_ecg_ii', 'signal_ecg_iii', 'PSG_Abdomen', 'PSG_Flow', 'PSG_Position', 'PSG_Snore', 'PSG_Thorax']],\n",
        "    df['event_encoded'],\n",
        "    test_size=0.2\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
        "\n",
        "# Reshape the data for CNN (assuming you have 1D data)\n",
        "input_shape = (X_train.shape[1], 1)"
      ],
      "metadata": {
        "id": "SE9Q5n6zl34-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Method"
      ],
      "metadata": {
        "id": "JYMqld_Qkgv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Dense, Embedding\n",
        "from keras.optimizers import Nadam\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset and preprocess it as needed\n",
        "# Assuming you have X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Train CNN model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv1D(32, kernel_size=3, activation='sigmoid', input_shape=(X_train.shape[1], 1)))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(128, activation='sigmoid'))\n",
        "cnn_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "cnn_model.compile(loss='sparse_categorical_crossentropy', optimizer=Nadam(learning_rate=0.001), metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCL36AUukaW9",
        "outputId": "9bf1e364-7171-4c10-84a7-14e34795bdf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "19766/19766 [==============================] - 56s 3ms/step - loss: 0.1706 - accuracy: 0.9281 - val_loss: 0.1496 - val_accuracy: 0.9357\n",
            "Epoch 2/10\n",
            "19766/19766 [==============================] - 55s 3ms/step - loss: 0.1426 - accuracy: 0.9379 - val_loss: 0.1412 - val_accuracy: 0.9392\n",
            "Epoch 3/10\n",
            "19766/19766 [==============================] - 54s 3ms/step - loss: 0.1357 - accuracy: 0.9421 - val_loss: 0.1362 - val_accuracy: 0.9417\n",
            "Epoch 4/10\n",
            "19766/19766 [==============================] - 53s 3ms/step - loss: 0.1316 - accuracy: 0.9442 - val_loss: 0.1380 - val_accuracy: 0.9404\n",
            "Epoch 5/10\n",
            "19766/19766 [==============================] - 53s 3ms/step - loss: 0.1285 - accuracy: 0.9459 - val_loss: 0.1308 - val_accuracy: 0.9452\n",
            "Epoch 6/10\n",
            "19766/19766 [==============================] - 54s 3ms/step - loss: 0.1255 - accuracy: 0.9472 - val_loss: 0.1288 - val_accuracy: 0.9466\n",
            "Epoch 7/10\n",
            "19766/19766 [==============================] - 54s 3ms/step - loss: 0.1230 - accuracy: 0.9482 - val_loss: 0.1248 - val_accuracy: 0.9494\n",
            "Epoch 8/10\n",
            "19766/19766 [==============================] - 55s 3ms/step - loss: 0.1205 - accuracy: 0.9494 - val_loss: 0.1251 - val_accuracy: 0.9480\n",
            "Epoch 9/10\n",
            "19766/19766 [==============================] - 53s 3ms/step - loss: 0.1185 - accuracy: 0.9502 - val_loss: 0.1210 - val_accuracy: 0.9500\n",
            "Epoch 10/10\n",
            "19766/19766 [==============================] - 53s 3ms/step - loss: 0.1165 - accuracy: 0.9511 - val_loss: 0.1220 - val_accuracy: 0.9486\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78abda9b5870>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LSTM model\n",
        "lstm_model = Sequential()\n",
        "\n",
        "# Reshape the data for LSTM\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "\n",
        "lstm_model.add(LSTM(64, activation='sigmoid', return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "lstm_model.add(LSTM(64, activation='sigmoid'))\n",
        "lstm_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer=Nadam(learning_rate=0.001), metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
        "\n",
        "\n",
        "# Make predictions with both models\n",
        "cnn_predictions = cnn_model.predict(X_test)\n",
        "lstm_predictions = lstm_model.predict(X_test)\n",
        "\n",
        "# Ensemble the predictions using simple averaging\n",
        "ensemble_predictions = (cnn_predictions + lstm_predictions) / 2\n",
        "\n",
        "# Convert predictions to class labels\n",
        "ensemble_labels = np.argmax(ensemble_predictions, axis=1)\n",
        "\n",
        "# Calculate accuracy of the ensemble model\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_labels)\n",
        "print(f'Ensemble Model Accuracy: {ensemble_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RCw8BBVsB5S",
        "outputId": "db5bfaa3-bf24-46a4-f2c6-b031f89498d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "19766/19766 [==============================] - 369s 18ms/step - loss: 0.2347 - accuracy: 0.9144 - val_loss: 0.1748 - val_accuracy: 0.9249\n",
            "Epoch 2/10\n",
            "19766/19766 [==============================] - 354s 18ms/step - loss: 0.1618 - accuracy: 0.9316 - val_loss: 0.1598 - val_accuracy: 0.9335\n",
            "Epoch 3/10\n",
            "19766/19766 [==============================] - 358s 18ms/step - loss: 0.1503 - accuracy: 0.9367 - val_loss: 0.1573 - val_accuracy: 0.9330\n",
            "Epoch 4/10\n",
            "19766/19766 [==============================] - 355s 18ms/step - loss: 0.1427 - accuracy: 0.9397 - val_loss: 0.1408 - val_accuracy: 0.9404\n",
            "Epoch 5/10\n",
            "19766/19766 [==============================] - 353s 18ms/step - loss: 0.1351 - accuracy: 0.9428 - val_loss: 0.1366 - val_accuracy: 0.9421\n",
            "Epoch 6/10\n",
            "19766/19766 [==============================] - 351s 18ms/step - loss: 0.1302 - accuracy: 0.9453 - val_loss: 0.1333 - val_accuracy: 0.9442\n",
            "Epoch 7/10\n",
            "19766/19766 [==============================] - 354s 18ms/step - loss: 0.1267 - accuracy: 0.9473 - val_loss: 0.1265 - val_accuracy: 0.9481\n",
            "Epoch 8/10\n",
            "19766/19766 [==============================] - 356s 18ms/step - loss: 0.1243 - accuracy: 0.9484 - val_loss: 0.1271 - val_accuracy: 0.9477\n",
            "Epoch 9/10\n",
            "19766/19766 [==============================] - 366s 19ms/step - loss: 0.1222 - accuracy: 0.9493 - val_loss: 0.1241 - val_accuracy: 0.9487\n",
            "Epoch 10/10\n",
            "19766/19766 [==============================] - 353s 18ms/step - loss: 0.1203 - accuracy: 0.9502 - val_loss: 0.1243 - val_accuracy: 0.9494\n",
            "2471/2471 [==============================] - 4s 1ms/step\n",
            "2471/2471 [==============================] - 15s 6ms/step\n",
            "Ensemble Model Accuracy: 0.9538463484354052\n"
          ]
        }
      ]
    }
  ]
}